---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: prom
  namespace: monitoring
spec:
  interval: 1m0s
  timeout: 15m0s
  chart:
    spec:
      chart: kube-prometheus-stack
      reconcileStrategy: ChartVersion
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      version: 56.6.2  # {"$imagepolicy": "flux-system:kube-prometheus-stack:tag"}
  valuesFrom:
    - kind: Secret
      name: cluster-info
      valuesKey: control-nodes
      targetPath: kubeControllerManager.endpoints
    - kind: Secret
      name: cluster-info
      valuesKey: control-nodes
      targetPath: kubeEtcd.endpoints
    - kind: Secret
      name: cluster-info
      valuesKey: control-nodes
      targetPath: kubeProxy.endpoints
    - kind: Secret
      name: cluster-info
      valuesKey: control-nodes
      targetPath: kubeScheduler.endpoints
  values:
    defaultRules:
      disabled:
        KubeClientCertificateExpiration: true
        KubePodCrashLooping: true
    additionalPrometheusRulesMap:
      kubernetes-resource-usage.rules:
        groups:
          - name: kubernetes-apps
            rules:
              - alert: KubePodCrashLooping
                annotations:
                  description: >-
                    Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{
                    $labels.container }}) is in waiting state (reason:
                    "CrashLoopBackOff").
                  runbook_url: >-
                    https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
                  summary: Pod is crash looping.
                expr: >-
                  max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff",
                  job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
                for: 5m
                labels:
                  severity: critical
          - name: kubernetes-rightsizing
            rules:
              - alert: PodNearMemoryLimit
                annotations:
                  description: >-
                    Pod {{ $labels.namespace }}/{{ $labels.pod }} is using
                    {{ $value }}% of memory limit for longer than 15 minutes.
                  summary: Pod memory usage above 90% of limit for more than 15 minutes.
                expr: |-
                  (
                    sum by (cluster, namespace, pod, container) (
                      avg_over_time(
                        container_memory_working_set_bytes{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[15m]
                      )
                    )
                  /
                    (
                      sum by (cluster, namespace, pod, container) (
                          kube_pod_container_resource_limits{job="kube-state-metrics",resource="memory"}
                      )
                    )
                  ) * 100 > 90
                for: 15m
                labels:
                  severity: critical
              - alert: PodHighCPUUsage
                annotations:
                  description: >-
                    Pod {{ $labels.namespace }}/{{ $labels.pod }} is using
                    {{ $value }}% of CPU request for longer than 15 minutes.
                  summary: Pod CPU usage has been high for more than 15 minutes.
                expr: |-
                  (
                    sum by (cluster, namespace, pod, container) (
                      rate(
                        container_cpu_usage_seconds_total{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[15m]
                      )
                    )
                  /
                    (
                      sum by (cluster, namespace, pod, container) (
                          kube_pod_container_resource_requests{job="kube-state-metrics",resource="cpu"} >= 0.1
                      )
                    )
                  ) * 100 > (100 + 100 / 3)
                for: 15m
                labels:
                  severity: critical
              - alert: LowCPUPodCPUUsageExceeded
                annotations:
                  description: >-
                    Pod {{ $labels.namespace }}/{{ $labels.pod }} is
                    overutilizing {{ $value }}% of low CPU request for longer
                    than 15 minutes.
                  summary: Pod CPU usage has been high for more than 15 minutes.
                expr: |-
                  (
                    sum by (cluster, namespace, pod, container) (
                      rate(
                        container_cpu_usage_seconds_total{image!="",job="kubelet",metrics_path="/metrics/cadvisor"}[15m]
                      )
                    )
                  /
                    (
                      sum by (cluster, namespace, pod, container) (
                          kube_pod_container_resource_requests{job="kube-state-metrics",resource="cpu"} <bool 0.1
                      )
                    ) == 1
                  ) * 100 > 10
                for: 15m
                labels:
                  severity: critical
          - name: temperature
            rules:
              - alert: TemperatureHigh
                annotations:
                  description: >-
                    Temperature has been {{ $value }} for more than 5 minutes
                    for {{ $labels.chip }} on {{ $labels.nodename }}
                  summary: Temperature alert - {{ $labels.nodename }}
                expr: (node_hwmon_temp_celsius > 75) * on (instance) group_left (nodename) node_uname_info{nodename=~".+"}
                for: 5m
                labels:
                  severity: critical

    alertmanager:
      ingress:
        enabled: true
        hosts:
          - alertmanager.kubi.${DOMAIN_LOCAL}
        annotations:
          hajimari.io/icon: mdi:bell-alert
          hajimari.io/appName: Alert Manager
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.tls: "true"
        labels:
          probe: enabled
      alertmanagerSpec:
        externalUrl: https://alertmanager.kubi.${DOMAIN_LOCAL}
        resources:
          requests:
            cpu: 5m
            memory: 40Mi
          limits:
            memory: 64Mi
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: longhorn
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 1Gi
        volumes:
          - name: alertmanager-secrets
            secret:
              secretName: alertmanager-secrets
              defaultMode: 420
        volumeMounts:
          - name: alertmanager-secrets
            readOnly: true
            mountPath: /etc/alertmanager/secrets/smtp-auth-password
            subPath: smtp-auth-password
      config:
        global:
          smtp_from: "AlertManager <alertmanager@${DOMAIN_PUBLIC}>"
          smtp_smarthost: "smtp.sendgrid.net:587"
          smtp_auth_username: apikey
          smtp_auth_password_file: /etc/alertmanager/secrets/smtp-auth-password
          resolve_timeout: 5m
        inhibit_rules:
          - source_matchers:
              - 'alertname!="PodHighMemoryUsage"'
            target_matchers:
              - 'alertname="PodNearMemoryLimit"'
            equal:
              - 'container'
              - 'pod'
          - source_matchers:
              - 'severity="critical"'
            target_matchers:
              - 'severity=~"warning|info"'
            equal:
              - 'namespace'
              - 'alertname'
          - source_matchers:
              - 'severity="warning"'
            target_matchers:
              - 'severity="info"'
            equal:
              - 'namespace'
              - 'alertname'
          - source_matchers:
              - 'alertname="InfoInhibitor"'
            target_matchers:
              - 'severity="info"'
            equal:
              - 'namespace'
        route:
          routes:
            - receiver: 'email'
              matchers:
                - severity = critical
            - receiver: 'email'
              matchers:
                - severity = "error"
                - namespace = "flux-system"
        receivers:
          - name: 'null'
          - name: 'email'
            email_configs:
              - to: "${EMAIL_ADDRESS}"
                send_resolved: true

    grafana:
      defaultDashboardsTimezone: "${TZ}"
      adminUser: null
      adminPassword: null
      admin:
        existingSecret: grafana-admin
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: 'default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          blackbox:
            gnetId: 13659
            revision: 1
            datasource: Prometheus
          cnpg:
            gnetId: 20417
            revision: 1
            datasource: Prometheus
          kubernetes-rightsizing:
            url: https://raw.githubusercontent.com/andrasmaroy/infra/main/kubernetes/home/foundation/monitoring/dashboards/kubernetes-rightsizing.json
            datasource: Prometheus
          node-temperatures:
            gnetId: 15202
            revision: 1
            datasource: Prometheus
          postgres:
            gnetId: 9628
            revision: 7
            datasource: Prometheus
          traefik:
            gnetId: 17347
            revision: 7
            datasource: Prometheus
          zfs-overview:
            url: https://raw.githubusercontent.com/45Drives/monitoring-stack/master/roles/grafana/files/zfs-overview.json
            datasource: Prometheus
          zfs-detailed-stats:
            url: https://raw.githubusercontent.com/45Drives/monitoring-stack/master/roles/grafana/files/zfs-detailed-stats.json
            datasource: Prometheus
      ingress:
        enabled: true
        hosts:
          - grafana.kubi.${DOMAIN_LOCAL}
        annotations:
          hajimari.io/icon: mdi:chart-bar-stacked
          hajimari.io/appName: Grafana
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.tls: "true"
        labels:
          probe: enabled
      resources:
        requests:
          cpu: 10m
          memory: 256Mi
        limits:
          memory: 384Mi
      sidecar:
        resources:
          requests:
            cpu: 1m
            memory: 100Mi
          limits:
            memory: 128Mi

    kubeScheduler:
      service:
        port: 10259
        targetPort: 10259
      serviceMonitor:
        https: true
        insecureSkipVerify: true

    kube-state-metrics:
      resources:
        limits:
          memory: 40Mi
        requests:
          cpu: 3m
          memory: 32Mi

    prometheus-node-exporter:
      nodeSelector:
        kubernetes.io/os: linux
        kubernetes.io/arch: amd64
      resources:
        limits:
          memory: 50Mi
        requests:
          cpu: 10m
          memory: 32Mi

    prometheusOperator:
      resources:
        limits:
          memory: 192Mi
        requests:
          cpu: 10m
          memory: 128Mi
      prometheusConfigReloader:
        resources:
          limits:
            memory: 50Mi
          requests:
            cpu: 1m
            memory: 32Mi

    prometheus:
      ingress:
        enabled: true
        hosts:
          - prometheus.kubi.${DOMAIN_LOCAL}
        annotations:
          hajimari.io/icon: mdi:database
          hajimari.io/appName: Prometheus
          traefik.ingress.kubernetes.io/router.entrypoints: websecure
          traefik.ingress.kubernetes.io/router.tls: "true"
        labels:
          probe: enabled
      prometheusSpec:
        image:
          tag: v2.44.0
        externalUrl: https://prometheus.kubi.${DOMAIN_LOCAL}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 7d
        retentionSize: 5GB
        nodeSelector:
          kubernetes.io/hostname: nas
          kubernetes.io/arch: amd64
        resources:
          requests:
            cpu: 250m
            memory: 4Gi
          limits:
            memory: 8Gi
        serviceMonitorSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: service-replicated-longhorn
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 10Gi
        additionalScrapeConfigs:
          - job_name: node-exporter/opnsense
            static_configs:
              - targets:
                  - ${ROUTER_IP}:9100
                labels:
                  job: node-exporter
            scrape_interval: 30s
            relabel_configs:
              - source_labels: [job]
                separator: ;
                regex: (.*)
                target_label: __tmp_prometheus_job_name
                replacement: $1
                action: replace
            metric_relabel_configs:
              - source_labels: [__name__]
                regex: node_memory_buffer_bytes
                target_label: __name__
                replacement: node_memory_Buffers_bytes
              - source_labels: [__name__]
                regex: node_memory_cache_bytes
                target_label: __name__
                replacement: node_memory_Cached_bytes
              - source_labels: [__name__]
                regex: node_memory_free_bytes
                target_label: __name__
                replacement: node_memory_MemFree_bytes
              - source_labels: [__name__]
                regex: node_memory_size_bytes
                target_label: __name__
                replacement: node_memory_MemTotal_bytes
              - source_labels: [__name__]
                regex: node_network_receive_drop_total
                target_label: __name__
                replacement: node_network_receive_drop_excluding_lo
              - source_labels: [__name__]
                regex: node_network_transmit_drop_total
                target_label: __name__
                replacement: node_network_transmit_drop_excluding_lo
              - source_labels: [__name__]
                regex: node_cpu_temperature_celsius
                target_label: __name__
                replacement: node_hwmon_temp_celsius
